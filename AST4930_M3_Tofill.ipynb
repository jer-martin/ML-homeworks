{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AST 4930 Module 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's load the scikit-learn module and load the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the data. Petal width vs. Petal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "norm = plt.Normalize(vmin=iris.target.min(), vmax=iris.target.max())\n",
    "\n",
    "setosa = ax.scatter(iris.data[:,2][iris.target == 1], iris.data[:,3][iris.target == 1], \n",
    "                    alpha=0.7, c=iris.target[iris.target == 1], cmap='viridis', norm=norm)\n",
    "\n",
    "versicolor = ax.scatter(iris.data[:,2][iris.target == 2], iris.data[:,3][iris.target == 2], \n",
    "                        alpha=0.7, c=iris.target[iris.target == 2], cmap='viridis', norm=norm)\n",
    "\n",
    "ax.set_xlim(2.5,7.5)\n",
    "ax.set_ylim(0.8,2.7)\n",
    "ax.set_xlabel(iris.feature_names[2])\n",
    "ax.set_ylabel(iris.feature_names[3])\n",
    "ax.legend([setosa, versicolor], iris.target_names[0:2], loc='upper left')\n",
    "\n",
    "plt.savefig('iris.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Using petal width/length only.\n",
    "X = iris.data[:,2:]\n",
    "y = (iris.target == 2).astype(np.float64)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# We need feature scaling for SVM.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Here I'm making three linear SVM Classification models with three different C values.\n",
    "svm_clf1 = LinearSVC(C=1, random_state=0)\n",
    "svm_clf2 = LinearSVC(C=100, random_state=0)\n",
    "svm_clf3 = LinearSVC(C=0.1, random_state=0)\n",
    "\n",
    "\n",
    "# Making pipelines.\n",
    "scaled_svm_clf1 = Pipeline([\n",
    "        (\"scaler\", scaler),\n",
    "        (\"linear_svc\", svm_clf1),\n",
    "    ])\n",
    "scaled_svm_clf2 = Pipeline([\n",
    "        (\"scaler\", scaler),\n",
    "        (\"linear_svc\", svm_clf2),\n",
    "    ])\n",
    "scaled_svm_clf3 = Pipeline([\n",
    "        (\"scaler\", scaler),\n",
    "        (\"linear_svc\", svm_clf3),\n",
    "    ])\n",
    "\n",
    "# Fit the data.\n",
    "scaled_svm_clf1.fit(X_train, y_train)\n",
    "scaled_svm_clf2.fit(X_train, y_train)\n",
    "scaled_svm_clf3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaled_svm_clf1.score(X_test, y_test),\n",
    "      scaled_svm_clf2.score(X_test, y_test),\n",
    "      scaled_svm_clf3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is to visualize the support vectors and margins.\n",
    "\n",
    "# Convert to unscaled parameters\n",
    "b1 = svm_clf1.decision_function([-scaler.mean_ / scaler.scale_])\n",
    "b2 = svm_clf2.decision_function([-scaler.mean_ / scaler.scale_])\n",
    "b3 = svm_clf3.decision_function([-scaler.mean_ / scaler.scale_])\n",
    "\n",
    "w1 = svm_clf1.coef_[0] / scaler.scale_\n",
    "w2 = svm_clf2.coef_[0] / scaler.scale_\n",
    "w3 = svm_clf3.coef_[0] / scaler.scale_\n",
    "\n",
    "svm_clf1.intercept_ = np.array([b1])\n",
    "svm_clf2.intercept_ = np.array([b2])\n",
    "svm_clf3.intercept_ = np.array([b3])\n",
    "\n",
    "svm_clf1.coef_ = np.array([w1])\n",
    "svm_clf2.coef_ = np.array([w2])\n",
    "svm_clf3.coef_ = np.array([w3])\n",
    "\n",
    "# Find support vectors (LinearSVC does not do this automatically). \n",
    "t = y * 2 - 1\n",
    "support_vectors_idx1 = (t * (X.dot(w1) + b1) < 1).ravel()\n",
    "support_vectors_idx2 = (t * (X.dot(w2) + b2) < 1).ravel()\n",
    "support_vectors_idx3 = (t * (X.dot(w3) + b3) < 1).ravel()\n",
    "\n",
    "svm_clf1.support_vectors_ = X[support_vectors_idx1]\n",
    "svm_clf2.support_vectors_ = X[support_vectors_idx2]\n",
    "svm_clf3.support_vectors_ = X[support_vectors_idx3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_boundary(svm_clf, xmin, xmax):\n",
    "    w = svm_clf.coef_[0]\n",
    "    b = svm_clf.intercept_[0]\n",
    "\n",
    "    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n",
    "    # => x1 = -w0/w1 * x0 - b/w1\n",
    "    x0 = np.linspace(xmin, xmax, 200)\n",
    "    decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n",
    "\n",
    "    margin = 1/w[1]\n",
    "    gutter_up = decision_boundary + margin\n",
    "    gutter_down = decision_boundary - margin\n",
    "\n",
    "    svs = svm_clf.support_vectors_\n",
    "    plt.scatter(svs[:, 0], svs[:, 1], s=120, edgecolors='black', facecolors='none', alpha=0.5)\n",
    "    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n",
    "    plt.plot(x0, gutter_up, \"k--\", linewidth=2)\n",
    "    plt.plot(x0, gutter_down, \"k--\", linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a plot for the model with C=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "norm = plt.Normalize(vmin=iris.target.min(), vmax=iris.target.max())\n",
    "\n",
    "setosa = ax.scatter(iris.data[:,2][iris.target == 1], iris.data[:,3][iris.target == 1], \n",
    "                    alpha=0.7, c=iris.target[iris.target == 1], cmap='viridis', norm=norm)\n",
    "\n",
    "versicolor = ax.scatter(iris.data[:,2][iris.target == 2], iris.data[:,3][iris.target == 2], \n",
    "                        alpha=0.7, c=iris.target[iris.target == 2], cmap='viridis', norm=norm)\n",
    "\n",
    "plot_svc_decision_boundary(svm_clf1, 2.5, 7.5)\n",
    "\n",
    "ax.set_xlim(2.5,7.5)\n",
    "ax.set_ylim(0.8,2.7)\n",
    "ax.set_xlabel(iris.feature_names[2])\n",
    "ax.set_ylabel(iris.feature_names[3])\n",
    "ax.legend([setosa, versicolor], iris.target_names[0:2], loc='upper left')\n",
    "\n",
    "plt.savefig('iris_SVM_C1.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a plot for the model with C=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "norm = plt.Normalize(vmin=iris.target.min(), vmax=iris.target.max())\n",
    "\n",
    "setosa = ax.scatter(iris.data[:,2][iris.target == 1], iris.data[:,3][iris.target == 1], \n",
    "                    alpha=0.7, c=iris.target[iris.target == 1], cmap='viridis', norm=norm)\n",
    "\n",
    "versicolor = ax.scatter(iris.data[:,2][iris.target == 2], iris.data[:,3][iris.target == 2], \n",
    "                        alpha=0.7, c=iris.target[iris.target == 2], cmap='viridis', norm=norm)\n",
    "\n",
    "plot_svc_decision_boundary(svm_clf2, 2.5, 7.5)\n",
    "\n",
    "\n",
    "ax.set_xlim(2.5,7.5)\n",
    "ax.set_ylim(0.8,2.7)\n",
    "ax.set_xlabel(iris.feature_names[2])\n",
    "ax.set_ylabel(iris.feature_names[3])\n",
    "ax.legend([setosa, versicolor], iris.target_names[0:2], loc='upper left')\n",
    "plt.savefig('iris_SVM_C100.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a plot for the model with C=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "norm = plt.Normalize(vmin=iris.target.min(), vmax=iris.target.max())\n",
    "\n",
    "setosa = ax.scatter(iris.data[:,2][iris.target == 1], iris.data[:,3][iris.target == 1], \n",
    "                    alpha=0.7, c=iris.target[iris.target == 1], cmap='viridis', norm=norm)\n",
    "\n",
    "versicolor = ax.scatter(iris.data[:,2][iris.target == 2], iris.data[:,3][iris.target == 2], \n",
    "                        alpha=0.7, c=iris.target[iris.target == 2], cmap='viridis', norm=norm)\n",
    "\n",
    "plot_svc_decision_boundary(svm_clf3, 2.5, 7.5)\n",
    "\n",
    "\n",
    "ax.set_xlim(2.5,7.5)\n",
    "ax.set_ylim(0.8,2.7)\n",
    "ax.set_xlabel(iris.feature_names[2])\n",
    "ax.set_ylabel(iris.feature_names[3])\n",
    "ax.legend([setosa, versicolor], iris.target_names[0:2], loc='upper left')\n",
    "plt.savefig('iris_SVM_C01.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:,2:]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "model = Pipeline([\n",
    "    ('sc', StandardScaler()),\n",
    "    ('svm', SVC(C=1, kernel=\"linear\", decision_function_shape='ovo', random_state=0))\n",
    "])\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compute the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine the minimum and maximum x & y ranges for the plot\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "print(x_min, x_max, y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = np.max([x_min, 0.])\n",
    "y_min = np.max([y_min, 0.])\n",
    "\n",
    "print(x_min, x_max, y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpts, ypts = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "Z = model.predict(np.c_[xpts.ravel(), ypts.ravel()])\n",
    "Z = Z.reshape(xpts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "ax.contourf(xpts, ypts, Z, alpha=0.5)\n",
    "\n",
    "norm = plt.Normalize(vmin=iris.target.min(), vmax=iris.target.max())\n",
    "\n",
    "#Let's over-plot training/test data points.\n",
    "plot_train = ax.scatter(X[:,0], X[:,1], alpha=0.7, c=y, cmap='viridis', \n",
    "                        norm=norm, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel(iris.feature_names[2])\n",
    "ax.set_ylabel(iris.feature_names[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification for non-linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "X,y = sklearn.datasets.make_circles(noise=0.1, factor=0.5, random_state=0)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")\n",
    "ax.set_xlim(-1.4,1.4)\n",
    "ax.set_ylim(-1.4,1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do the data look when mapped onto a 3D space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# add the squared first feature\n",
    "X_new = np.hstack([X, X[:,0:1]**2 + X[:, 1:2] ** 2])\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D, axes3d\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# visualize in 3D\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# plot first all the points with y==0, then all with y == 1\n",
    "mask = y == 0\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], \n",
    "           s=60, edgecolor='k')\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], marker='^',\n",
    "           s=60, edgecolor='k')\n",
    "\n",
    "ax.set_xlim(-1.4,1.4)\n",
    "ax.set_ylim(-1.4,1.4)\n",
    "ax.set_xlabel(\"feature1\")\n",
    "ax.set_ylabel(\"feature2\")\n",
    "ax.set_zlabel(\"feature1**2 + feature2**2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Here I'm using a linear version of SVC.\n",
    "linear_svm_3d = LinearSVC().fit(X_new, y)\n",
    "coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n",
    "\n",
    "# Show linear decision boundary.\n",
    "fig = plt.figure()\n",
    "\n",
    "# visualize in 3D\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "xx = np.linspace(X_new[:, 0].min() - 0.5, X_new[:, 0].max() + 0.5, 100)\n",
    "yy = np.linspace(X_new[:, 1].min() - 0.5, X_new[:, 1].max() + 0.5, 100)\n",
    "\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n",
    "\n",
    "ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3, color='gray')\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], \n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], marker='^',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "\n",
    "ax.set_xlim(-1.4,1.4)\n",
    "ax.set_ylim(-1.4,1.4)\n",
    "ax.set_xlabel(\"feature1\")\n",
    "ax.set_ylabel(\"feature2\")\n",
    "ax.set_zlabel(\"feature1**2 + feature2**2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is how the hyperplane looks like in the original 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZZ = XX**2 + YY**2\n",
    "dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\n",
    "             cmap=mglearn.cm2, alpha=0.5)\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")\n",
    "ax.set_xlim(-1.4,1.4)\n",
    "ax.set_ylim(-1.4,1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do kNN and DT perform on the same problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's kNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_neighbors': np.arange(10)+1,\n",
    "              'weights': ['uniform','distance']}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, return_train_score=True, \n",
    "                           verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best model: {}\".format(grid_search.best_estimator_))\n",
    "print(\"Test score: {:.2f}\".format(grid_search.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(X_new[:, 0].min() - 0.5, X_new[:, 0].max() + 0.5, 100)\n",
    "yy = np.linspace(X_new[:, 1].min() - 0.5, X_new[:, 1].max() + 0.5, 100)\n",
    "\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "\n",
    "#This is to make a data structure that is consistent with the training/test datasets.\n",
    "Z = grid_search.best_estimator_.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "#Now let's reshape to match with the meshgrid.\n",
    "Z = Z.reshape(XX.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.contourf(XX, YY, Z, alpha=0.5, cmap=mglearn.cm2)\n",
    "\n",
    "#Let's over-plot training/test data points.\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "ax.set_xlabel('feature 1')\n",
    "ax.set_ylabel('feature 2')\n",
    "ax.set_xlim(-1.4,1.4)\n",
    "ax.set_ylim(-1.4,1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's DT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_depth': np.arange(10)+1,\n",
    "              'criterion': ['gini','entropy']}\n",
    "\n",
    "# By defaults, sklearn's GridSearchCV will use stratified k-fold for classification problems.\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, return_train_score=True, \n",
    "                           verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best model: {}\".format(grid_search.best_estimator_))\n",
    "print(\"Test score: {:.2f}\".format(grid_search.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(X_new[:, 0].min() - 0.5, X_new[:, 0].max() + 0.5, 100)\n",
    "yy = np.linspace(X_new[:, 1].min() - 0.5, X_new[:, 1].max() + 0.5, 100)\n",
    "\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "\n",
    "#This is to make a data structure that is consistent with the training/test datasets.\n",
    "Z = grid_search.best_estimator_.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "#Now let's reshape to match with the meshgrid.\n",
    "Z = Z.reshape(XX.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.contourf(XX, YY, Z, alpha=0.5, cmap=mglearn.cm2)\n",
    "\n",
    "#Let's over-plot training/test data points.\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "ax.set_xlabel('feature 1')\n",
    "ax.set_ylabel('feature 2')\n",
    "ax.set_xlim(-1.4,1.4)\n",
    "ax.set_ylim(-1.4,1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try SVM on another non-linear problem. This time two moons instead of two circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# This is what it looks like without noise.\n",
    "X, y = sklearn.datasets.make_moons(n_samples=200, noise=0., random_state=0)\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Now I'm adding some noise.\n",
    "X,y = sklearn.datasets.make_moons(n_samples=200, noise=0.3, random_state=0)\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Run GridSearchCV and find the best SVM model. We will use \"rbf\" kernel and vary hyperparameter 'C' and 'gamma'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Let's make a heatmap and see if we are converged with the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Using the best model, plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is your best hyperpameters? What is your test score? How does the decision boundary look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do kNN and DT do on this problem? Choose kNN or DT and complete the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_neighbors': np.arange(30)+1,\n",
    "              'weights': ['uniform','distance']}\n",
    "\n",
    "# By defaults, sklearn's GridSearchCV will use stratified k-fold for classification problems.\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, return_train_score=True, \n",
    "                           verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best model: {}\".format(grid_search.best_estimator_))\n",
    "print(\"Test score: {:.2f}\".format(grid_search.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, 100)\n",
    "yy = np.linspace(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5, 100)\n",
    "\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "\n",
    "#This is to make a data structure that is consistent with the training/test datasets.\n",
    "Z = grid_search.best_estimator_.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "#Now let's reshape to match with the meshgrid.\n",
    "Z = Z.reshape(XX.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.contourf(XX, YY, Z, alpha=0.5, cmap=mglearn.cm2)\n",
    "\n",
    "#Let's over-plot training/test data points.\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "ax.set_xlabel('feature 1')\n",
    "ax.set_ylabel('feature 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is with DT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'max_depth': np.arange(30)+1,\n",
    "              'criterion': ['gini','entropy']}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid, cv=5, return_train_score=True, \n",
    "                           verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best model: {}\".format(grid_search.best_estimator_))\n",
    "print(\"Test score: {:.2f}\".format(grid_search.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5, 100)\n",
    "yy = np.linspace(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5, 100)\n",
    "\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "Z = grid_search.best_estimator_.predict(np.c_[XX.ravel(), YY.ravel()])\n",
    "Z = Z.reshape(XX.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "ax.contourf(XX, YY, Z, alpha=0.5, cmap=mglearn.cm2)\n",
    "\n",
    "#Let's over-plot training/test data points.\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "\n",
    "ax.set_xlabel('feature 1')\n",
    "ax.set_ylabel('feature 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load and split the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find the best SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Remember that we need feature scaling for SVM.\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('SVM', SVC(kernel='rbf'))\n",
    "])\n",
    "\n",
    "# Choose hyperparameters to optimize.\n",
    "param_grid = {'SVM__C': [0.01, 0.1, 1., 10., 100.],\n",
    "              'SVM__gamma': [0.01, 0.1, 1., 10., 100.]}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=5, return_train_score=True, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best model: {}\".format(grid_search.best_estimator_))\n",
    "print(\"Test score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compute the permutation importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first fit our best model.\n",
    "\n",
    "model = grid_search.best_estimator_[1]\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# We then compute the permutation importance using the test data.\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "r = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's print out what's in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a feature importance plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_importances(model):\n",
    "    n_features = X.shape[1]\n",
    "    plt.barh(np.arange(n_features), r.importances_mean, align='center')\n",
    "    plt.yticks(np.arange(n_features), iris.feature_names)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)\n",
    "\n",
    "plot_feature_importances(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also make a box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = r.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.boxplot(r.importances[sorted_idx].T,\n",
    "           vert=False, labels=np.array(iris.feature_names)[sorted_idx])\n",
    "\n",
    "ax.set_xlabel(\"Permutation importance\")\n",
    "ax.set_ylabel(\"Feature\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
